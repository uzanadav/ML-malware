install.packages("MASS", repos = "http://cran.us.r-project.org")
install.packages("fitdistrplus", repos = "http://cran.us.r-project.org")
install.packages("magrittr", repos = "http://cran.us.r-project.org")
install.packages("dplyr", repos = "http://cran.us.r-project.org")
install.packages("e1071", repos = "http://cran.us.r-project.org")
install.packages("plotly", repos = "http://cran.us.r-project.org")
install.packages("ggplot2", repos = "http://cran.us.r-project.org")
install.packages("knitr", repos = "http://cran.us.r-project.org")
install.packages("sqldf", repos = "http://cran.us.r-project.org")
install.packages("readr", repos = "http://cran.us.r-project.org")
install.packages("caret", repos = "http://cran.us.r-project.org")
install.packages("tidyr", repos = "http://cran.us.r-project.org")

library(MASS)
library(fitdistrplus)
library(magrittr)
library(dplyr)
library(e1071)
library(plotly)
library(ggplot2)
library(sqldf)
library(knitr)
library(readr)
library(Matrix)
library(sqldf)
library(scales)
library(caTools)
library(klaR)
library(clustMixType)
library(gridExtra)
library(clv)
library(mvtnorm)
library(fields)
library(cluster)
library(cclust)
library(cluster)
library(topicmodels)
library(corrplot)
library(data.table)
library(dplyr)
library(tidyr)
library(caret)

library(caTools)

##------------------------- Load The Database-------------------------------------------------------
Traindata<-read.csv('trainData_little (1).csv')
##-------------------------- Spliting The Data------------------------------------------------------
set.seed(123)
split <- sample.split(Traindata, SplitRatio =  0.8) #decide of that is the right value
training_set <- subset(Traindata, split==T)
test_set <- subset(Traindata, split==F)

##-------------------------- DT--------------CP Defualt---------------------------------------------
tree <- rpart(training_set$y1~. , data = training_set, method = "class", parm=list(split='information'))
rpart.plot(tree, extra=104, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE,)

printcp(tree)
y_pred1<- predict(tree, newdata = test_set[-32],type = 'class')
y_pred2 <- predict(tree, newdata = training_set[-32],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)
##---------------------CP=0-------------------------------------------------------------------------
tree2 <- rpart(training_set$y1~. , data = training_set, method = "class", parm=list(split='information'),cp=0)
rpart.plot(tree2, extra=104, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE,)
printcp(tree2)
plotcp(tree2,cex.main=100,cex.lab=2,cex.axis=1.5)

(opt <- which.min(tree2$cptable[,"xerror"])) ##finding the opt cp 
(cp <- tree2$cptable[opt, "CP"]) #get its value
##---------------------CP=optimal-------------------------------------------------------------------------
tree3 <- rpart(training_set$y1~. , data = training_set, method = "class", parm=list(split='information'),cp=cp)
rpart.plot(tree3, extra=104, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE,)
 
printcp(tree3)


y_pred1<- predict(tree3, newdata = test_set[-32],type = 'class')
y_pred2 <- predict(tree3, newdata = training_set[-32],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

tree3 <- rpart(training_set$y1~. , data = training_set, method = "class", parm=list(split='information'),cp=0.00238156)
rpart.plot(tree3, extra=104, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE)

#-----------------------------------Tracking one record manually --------------------------

record_trace <-0
samplerow <-test_set[sample(nrow(test_set), 1) ] #row tested
print(samplerow)
rowPred  <- predict(tree3, newdata=samplerow[,],type = 'class') #prediction tree "answer" by probs
cmrow = table(samplerow$y1, rowPred)##           confuison matrix
paste(rowPred)

# calculate accuracy
preds_record_trace ==row_of_record$Y #test accuracy

##-----------------------------veriable importance ----------------------------------
tree3$variable.importance
##-------------------------CP---------------------------------------------

tree4 <- rpart(training_set$y1~. , data = training_set, method = "class", parm=list(split='information'),cp=0.88389895)
rpart.plot(tree4, extra=104, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE,)

printcp(tree4)


y_pred1<- predict(tree2, newdata = test_set[-32],type = 'class')
y_pred2 <- predict(tree2, newdata = training_set[-32],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)



tree5 <- rpart(training_set$y1~. , data = training_set, method = "class", parm=list(split='information'),cp=0.00063792)
rpart.plot(tree5, extra=104, box.palette="GnBu",branch.lty=3, shadow.col="gray", nn=TRUE,)

printcp(tree5)


y_pred1<- predict(tree5, newdata = test_set[-32],type = 'class')
y_pred2 <- predict(tree5, newdata = training_set[-32],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

##----------------------------------NN------------------------------------------------------------------------------------------------------------

summary(Traindata$X3)




Traindata$X2 = factor(Traindata$X2,
                                 levels = c('http', 'private', 'smtp','smtp','other'),
                                 labels = c('http', 'private', 'smtp','smtp','other'))

Traindata$X2[is.na(Traindata$X2) ] <- 'other'

Traindata$X3 = factor(Traindata$X3,
                      levels = c('SF', 'S0', 'REJ'),
                      labels = c('SF', 'S0', 'REJ'))

Traindata$X3 <- ifelse(is.na(Traindata$X3), 
                            'other',Traindata$X3) 

Traindata$X3 = factor(Traindata$X3,
                      levels = c(1,2,3,'other' ),
                      labels = c('SF', 'S0', 'REJ','other'))


a <- dummyVars(" ~ .",data=Traindata)
Traindata <- data.frame(predict(a, newdata = Traindata))



Traindata$X1.icmp <- factor(with(Traindata,ifelse((Traindata$X1.icmp==1),1,-1)))
Traindata$X1.tcp <- factor(with(Traindata,ifelse((Traindata$X1.tcp==1),1,-1)))
Traindata$X1.udp <- factor(with(Traindata,ifelse((Traindata$X1.udp==1),1,-1)))

Traindata$X2.http <- factor(with(Traindata,ifelse((Traindata$X2.http==1),1,-1)))
Traindata$X2.private <- factor(with(Traindata,ifelse((Traindata$X2.private==1),1,-1)))
Traindata$X2.smtp <- factor(with(Traindata,ifelse((Traindata$X2.smtp==1),1,-1)))
Traindata$X2.other <- factor(with(Traindata,ifelse((Traindata$X2.other==1),1,-1)))

Traindata$X3.SF <- factor(with(Traindata,ifelse((Traindata$X3.SF==1),1,-1)))
Traindata$X3.S0<- factor(with(Traindata,ifelse((Traindata$X3.S0==1),1,-1)))
Traindata$X3.REJ <- factor(with(Traindata,ifelse((Traindata$X3.REJ==1),1,-1)))
Traindata$X3.other <- factor(with(Traindata,ifelse((Traindata$X3.other==1),1,-1)))

##------------------------------scale-------------------------------------------------------------------------------
library(Matrix)
TraindataMat <- data.matrix(sqldf("select * from Traindata"))

maxs_train <- as.numeric(apply(TraindataMat, 2, max))
mins_train <- as.numeric(apply(TraindataMat, 2, min))

TraindataMat <- as.data.frame(scale(TraindataMat, center = mins_train, scale = maxs_train - mins_train))

Traindata <- TraindataMat

Traindata$X1.icmp <- factor(with(Traindata,ifelse((Traindata$X1.icmp==1),1,-1)))
Traindata$X1.tcp <- factor(with(Traindata,ifelse((Traindata$X1.tcp==1),1,-1)))
Traindata$X1.udp <- factor(with(Traindata,ifelse((Traindata$X1.udp==1),1,-1)))

Traindata$X2.http <- factor(with(Traindata,ifelse((Traindata$X2.http==1),1,-1)))
Traindata$X2.private <- factor(with(Traindata,ifelse((Traindata$X2.private==1),1,-1)))
Traindata$X2.smtp <- factor(with(Traindata,ifelse((Traindata$X2.smtp==1),1,-1)))
Traindata$X2.other <- factor(with(Traindata,ifelse((Traindata$X2.other==1),1,-1)))

Traindata$X3.SF <- factor(with(Traindata,ifelse((Traindata$X3.SF==1),1,-1)))
Traindata$X3.S0<- factor(with(Traindata,ifelse((Traindata$X3.S0==1),1,-1)))
Traindata$X3.REJ <- factor(with(Traindata,ifelse((Traindata$X3.REJ==1),1,-1)))
Traindata$X3.other <- factor(with(Traindata,ifelse((Traindata$X3.other==1),1,-1)))


set.seed(123)
split <- sample.split(Traindata, SplitRatio =  0.8) #decide of that is the right value
training_set <- subset(Traindata, split==T)
test_set <- subset(Traindata, split==F)


##--------------------NN training date ------------------------------------------------- 

install.packages("nnet", repos = "http://cran.us.r-project.org")
library(nnet)
x <- training_set[,1:38]
y <- class.ind(training_set[,39])
nn <- nnet(x=training_set[,1:38], y=class.ind(training_set[,39]), size=1, linout=FALSE, softmax=T) # train

y_pred2<- predict(nn, newdata = training_set[,1:38],type = 'class')
y_pred1 <- predict(nn, newdata = test_set[,1:38],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

#----------------------Plot NN---------------------------------------------
install.packages("clusterGeneration", repos = "http://cran.us.r-project.org")
install.packages("devtools", repos = "http://cran.us.r-project.org")
install.packages("nnet", repos = "http://cran.us.r-project.org")
library(devtools)
library(nnet)
library(clusterGeneration)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot.nnet(nn)
olden(nn,out_var='1')

##-------------------------------optimum NN -------------------------------- 

#devide train to folds
set.seed(234)
temp <- runif(nrow(training_set))
data <-training_set[order(temp),]
folds <- sample(4,nrow(data), replace= TRUE)
#View(folds)

nnum <- seq(1,20,1)
nnum <- c(nnum,2)# check specific case 
#print(nnum)
acc <- c()
set.seed(2341)

for(n in nnum){
  acc_i <- c()
  for (i in 1:4){
    nn_train <- rbind(data[folds != i,])
    n_validation <- data[folds == i,]
    NN_opt <- nnet(x=nn_train[,1:38], y=class.ind(nn_train[,39]), size=n, linout=FALSE, softmax=T, MaxNWts = 3340) 
    preds.nn.val<- factor(predict(NN_opt, newdata=n_validation[,1:38], type='class'))
    acc_res <- (sum((preds.nn.val == n_validation$y1))/nrow(n_validation))
    acc_i <- c(acc_i,acc_res)
  }
  acc <- c(acc, mean(acc_i))
}

#Plot Accuracy by number of neurons
(acc)
ggplot(data.frame(x=nnum, y=acc)) + geom_point(aes(x,y), color='purple',cex=4) +
  xlab("Number of Neurons") + ylab("Validation Accuracy")+labs(title="Accuracy by number of neurons")+
  theme(axis.text=element_text(size=20),
        axis.title=element_text(size=20,face="bold"))
optimum_n <- nnum[which.max(acc)]
print(optimum_n)

# -------------------------------Building optimal network--------------------------

NN_opt <- nnet(x=nn_train[,1:38][,1:35], y=class.ind(nn_train[,39]), size=6, linout=FALSE, softmax=T) # train

y_pred2<- predict(NN_opt, newdata = training_set[,1:38],type = 'class')
y_pred1 <- predict(NN_opt, newdata = test_set[,1:38],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

#------------------------------Check Weighted Decay Infulance------------
decay<-seq(0.1,1,0.1)
acc<-c()
for(dec in decay){
  acc_i <- c()
  for (i in 1:4){
    nn_train <- rbind(data[folds != i,])
    nn_validation <- data[folds == i,]
    nn <- nnet(x=nn_train[,1:38], y=class.ind(nn_train[,39]), size=6, linout=FALSE, softmax=T, decay = dec) 
    preds.nn.val<- factor(predict(nn, newdata=nn_validation[,1:38], type='class'))
    acc_res <- (sum((preds.nn.val == nn_validation$y1))/nrow(nn_validation))
    acc_i <- c(acc_i,acc_res)
  }
  acc <- c(acc, mean(acc_i))
}

View(acc)


ggplot(data.frame(x=decay, y=acc)) + geom_point(aes(x,y), color='purple',cex=4) +
  xlab("decay") + ylab("Validation Accuracy")+labs(title="Accuracy by number of neurons")+
  theme(axis.text=element_text(size=20),
        axis.title=element_text(size=20,face="bold"))
optimum_decay <- decay[which.max(acc)]

NN_opt2 <- nnet(x=nn_train[,1:38][,1:35], y=class.ind(nn_train[,39]), size=6, linout=FALSE, softmax=T, decay = optimum_decay) # train

y_pred2<- predict(NN_opt2, newdata = training_set[,1:38],type = 'class')
y_pred1 <- predict(NN_opt2, newdata = test_set[,1:38],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

#------------------------------------------------- check optimum ittraitions 

iter<-seq(50,300,50)
acc<-c()
for(iters in iter){
  acc_i <- c()
  
  for (i in 1:4){
    nn_train <- rbind(data[folds != i,])
    nn_validation <- data[folds == i,]
    nn <- nnet(x=nn_train[,1:38], y=class.ind(nn_train[,39]), size=6,decay = optimum_decay, linout=FALSE, softmax=T, maxit = iters) 
    preds.nn.val<- factor(predict(nn, newdata=nn_validation[,1:38], type='class'))
    acc_res <- (sum((preds.nn.val == nn_validation$y1))/nrow(nn_validation))
    acc_i <- c(acc_i,acc_res)
  }
  acc <- c(acc, mean(acc_i))
}

ggplot(data.frame(x=iter, y=acc)) + geom_point(aes(x,y), color='purple',cex=4) +
  xlab("Max Num Of Iter") + ylab("Validation Accuracy")+labs(title="Accuracy by Max Num Of Iter")+
  theme(axis.text=element_text(size=20),
        axis.title=element_text(size=20,face="bold"))

optimum_iter <- iter[which.max(acc)]

NN_opt3 <- nnet(x=nn_train[,1:38][,1:35], y=class.ind(nn_train[,39]), size=6, linout=FALSE, softmax=T, decay = optimum_decay,maxit=optimum_iter) # train

y_pred2<- predict(NN_opt3, newdata = training_set[,1:38],type = 'class')
y_pred1 <- predict(NN_opt3, newdata = test_set[,1:38],type = 'class')

cm1 = table(test_set$y1, y_pred1)##           confuison matrix
cm2=table(training_set$y1, y_pred2)
true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

##----------------------------------------k means----------------------------------------
install.packages("MASS", repos = "http://cran.us.r-project.org")
install.packages("fitdistrplus", repos = "http://cran.us.r-project.org")
install.packages("magrittr", repos = "http://cran.us.r-project.org")
install.packages("dplyr", repos = "http://cran.us.r-project.org")
install.packages("e1071", repos = "http://cran.us.r-project.org")
install.packages("plotly", repos = "http://cran.us.r-project.org")
install.packages("ggplot2", repos = "http://cran.us.r-project.org")
install.packages("knitr", repos = "http://cran.us.r-project.org")
install.packages("sqldf", repos = "http://cran.us.r-project.org")
install.packages("readr", repos = "http://cran.us.r-project.org")
install.packages("caret", repos = "http://cran.us.r-project.org")
install.packages("tidyr", repos = "http://cran.us.r-project.org")
install.packages("clv", repos = "http://cran.us.r-project.org")
install.packages("fields", repos = "http://cran.us.r-project.org")
install.packages("cluster", repos = "http://cran.us.r-project.org")
install.packages("topicmodels", repos = "http://cran.us.r-project.org")
install.packages("corrplot", repos = "http://cran.us.r-project.org")
install.packages("cclust", repos = "http://cran.us.r-project.org")
install.packages("stats", repos = "http://cran.us.r-project.org")
install.packages("caret", repos = "http://cran.us.r-project.org")


library(stats)
library(klaR)
library(clustMixType)
library(ggplot2)
library(gridExtra)
library(clv)
library(mvtnorm)
library(fields)
library(cluster)
library(clv)
library(cclust)
library(cluster)
library(topicmodels)
library(corrplot)
library(caret)
library(sqldf)



Traindata<-read.csv('trainData_little (1).csv')
Y <-  Traindata[,31]



Traindata$X2 = factor(Traindata$X2,
                      levels = c('http', 'private', 'smtp','smtp','other'),
                      labels = c('http', 'private', 'smtp','smtp','other'))

Traindata$X2[is.na(Traindata$X2) ] <- 'other'

Traindata$X3 = factor(Traindata$X3,
                      levels = c('SF', 'S0', 'REJ'),
                      labels = c('SF', 'S0', 'REJ'))

Traindata$X3 <- ifelse(is.na(Traindata$X3), 
                       'other',Traindata$X3) 

Traindata$X3 = factor(Traindata$X3,
                      levels = c(1,2,3,'other' ),
                      labels = c('SF', 'S0', 'REJ','other'))


a <- dummyVars(" ~ .",data=Traindata)
Traindata <- data.frame(predict(a, newdata = Traindata))



Traindata$X1.icmp <- factor(with(Traindata,ifelse((Traindata$X1.icmp==1),1,-1)))
Traindata$X1.tcp <- factor(with(Traindata,ifelse((Traindata$X1.tcp==1),1,-1)))
Traindata$X1.udp <- factor(with(Traindata,ifelse((Traindata$X1.udp==1),1,-1)))

Traindata$X2.http <- factor(with(Traindata,ifelse((Traindata$X2.http==1),1,-1)))
Traindata$X2.private <- factor(with(Traindata,ifelse((Traindata$X2.private==1),1,-1)))
Traindata$X2.smtp <- factor(with(Traindata,ifelse((Traindata$X2.smtp==1),1,-1)))
Traindata$X2.other <- factor(with(Traindata,ifelse((Traindata$X2.other==1),1,-1)))

Traindata$X3.SF <- factor(with(Traindata,ifelse((Traindata$X3.SF==1),1,-1)))
Traindata$X3.S0<- factor(with(Traindata,ifelse((Traindata$X3.S0==1),1,-1)))
Traindata$X3.REJ <- factor(with(Traindata,ifelse((Traindata$X3.REJ==1),1,-1)))

##----------------------------scale------------------------------
TraindataMat <- data.matrix(sqldf("select * from Traindata"))

maxs_train <- as.numeric(apply(TraindataMat, 2, max))
mins_train <- as.numeric(apply(TraindataMat, 2, min))

TraindataMat <- as.data.frame(scale(TraindataMat, center = mins_train, scale = maxs_train - mins_train))

Traindata <- TraindataMat

Traindata$X1.icmp <- factor(with(Traindata,ifelse((Traindata$X1.icmp==1),1,-1)))
Traindata$X1.tcp <- factor(with(Traindata,ifelse((Traindata$X1.tcp==1),1,-1)))
Traindata$X1.udp <- factor(with(Traindata,ifelse((Traindata$X1.udp==1),1,-1)))

Traindata$X2.http <- factor(with(Traindata,ifelse((Traindata$X2.http==1),1,-1)))
Traindata$X2.private <- factor(with(Traindata,ifelse((Traindata$X2.private==1),1,-1)))
Traindata$X2.smtp <- factor(with(Traindata,ifelse((Traindata$X2.smtp==1),1,-1)))
Traindata$X2.other <- factor(with(Traindata,ifelse((Traindata$X2.other==1),1,-1)))

Traindata$X3.SF <- factor(with(Traindata,ifelse((Traindata$X3.SF==1),1,-1)))
Traindata$X3.S0<- factor(with(Traindata,ifelse((Traindata$X3.S0==1),1,-1)))
Traindata$X3.REJ <- factor(with(Traindata,ifelse((Traindata$X3.REJ==1),1,-1)))
Traindata$X3.other <- factor(with(Traindata,ifelse((Traindata$X3.other==1),1,-1)))

##-------------------defualt k means 
set.seed(123)
split <- sample.split(Traindata, SplitRatio =  0.8) #decide of that is the right value
K_training <- subset(Traindata, split==T)
K_test_set <- subset(Traindata, split==F)

Y_train <- K_training[,39]
Y_test <- K_test_set[,39]
K_training <- K_training[,1:38]
K_test_set <- K_test_set[,1:38]
##----------------------defualt K ----------------------------- 
k <- 2
inter <- c(); outer <- c(); ratio <- c();
K_trainingmat <- data.matrix(K_training)
K_test_setmat <- data.matrix(K_test_set)

clust_data<-cclust(K_trainingmat, center=k ,method = 'kmeans')


y_predTrain<- predict(clust_data, newdata = K_trainingmat,type = 'class')
y_predTest <- predict(clust_data, newdata = K_test_setmat,type = 'class')


accuracy1 <-(sum((y_predTrain$cluster-1)==Y_train))/nrow(K_training)
paste("Train accuracy ", accuracy1)

accuracy2 <-(sum((y_predTest$cluster-1)==Y_test))/nrow(K_test_set)
paste("Tesst accuracy ", accuracy2)


##-----------------------------------------------------------------------

clust_data<- kmeans(K_trainingmat , centers= k, iter.max=30, nstart=5)
scatt_data<-cls.scatt.data(K_trainingmat, clust=clust_data$cluster, dist='euclidean')

inter <- clust_data$tot.withinss
outer <- clust_data$betweenss
ratio <- clust_data$tot.withinss/clust_data$betweenss

dunn  <- clv.Dunn(scatt_data, 'centroid', 'centroid')
paste("the value of dunn ", dunn)
DB  <- clv.Davies.Bouldin(scatt_data, 'centroid', 'centroid')
paste("the value of DB ",DB)

library(cluster)

Traindata <- data.matrix(Traindata)

ggplot(Traindata, aes(x=Traindata$X23, y=Y,  color=clust_data$cluster, size=10)) + geom_point() + guides(color=F, size=F) 

clusplot(x=K_training, main="Kmeans train", y_predTrain$cluster, color=TRUE, shade=TRUE, labels=0, lines=0)
##--------------------------------best K -----------------------------

dunn <- c(); DB <- c(); K <- 10 
for(k in 2:K){
  if(k!=3){
    clust_data    <- kmeans(K_trainingmat, centers=k, iter.max=25, nstart=5)
    scatt_data    <- cls.scatt.data(K_trainingmat, clust=clust_data$cluster, dist='euclidean')
    dunn          <- c(dunn, clv.Dunn(scatt_data, 'centroid', 'centroid'))
    DB            <- c(DB,   clv.Davies.Bouldin(scatt_data, 'centroid', 'centroid'))
  }}
max(dunn)
min(DB)
clust_metrics <- data.frame(K = c(2,rep(seq(4,K),1)), value = c(dunn, DB), metric = c(rep('Dunn',length1), rep('DB',length1)))
ggplot(clust_metrics, aes(x=K, y=value, color=factor(metric))) + geom_point() + geom_line()

print(dunn)
print(DB)

ratio <- DB-dunn
print(ratio)

##-------------------------------HC------------------------------------------
# Fitting Hierarchical Clustering to the dataset
hc1 = hclust(d = dist(K_training, method = 'euclidean'), method = 'ward.D')
y_hc1 = cutree(hc1, 5)

hc2 = hclust(d = dist(K_test_set, method = 'euclidean'), method = 'ward.D')
y_hc2 = cutree(hc, 5)



cm1 = table(Y_train, y_hc1)##           confuison matrix
cm2 = table(Y_test, y_hc2)##           confuison matrix

true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

##-----------------------------chosen model -----------

NN_opt3 <- nnet(x=Traindata[,1:38], y=class.ind(Traindata[,39]), size=6, linout=FALSE, softmax=T, decay = optimum_decay,maxit=optimum_iter) # train

y_pred2<- predict(NN_opt3, newdata = Traindata[,1:38],type = 'class')


cm1 = table(y_pred2, Traindata$y1)##           confuison matrix


true1 <- cm1[1,1]+cm1[2,2]
False1 <- cm1[1,2]+cm1[2,1]
accuracy1 <- true1/(true1+False1)

true2 <- cm2[1,1]+cm2[2,2]
False2 <- cm2[1,2]+cm2[2,1]
accuracy2 <- true2/(true2+False2)
paste("Training accuarcy is : ",accuracy1,"Test accuarcy is : ",accuracy2)

##----------------------X-test predications ---------------------------------

TestData<-read.csv('X_test.csv')

TestData$X24[which(TestData$X24>0&TestData$X24<1)] <- 0.5
  TestData$X25[which(TestData$X25>0&TestData$X25<1)] <- 0.5



TestData <-sqldf("select X0,X1,X2,X3,X4,X6,X7,X9,X10,X11,X12,X13,X17,X18,X23,X24,X25,X26,X29,X30,X31,X32,X33,X34,X35,X36,X37,X38,X39,X40 
              from TestData ")


TestData$X2 = factor(TestData$X2,
                      levels = c('http', 'private', 'smtp','smtp','other'),
                      labels = c('http', 'private', 'smtp','smtp','other'))

TestData$X2[is.na(TestData$X2) ] <- 'other'

TestData$X3 = factor(TestData$X3,
                      levels = c('SF', 'S0', 'REJ'),
                      labels = c('SF', 'S0', 'REJ'))

TestData$X3 <- ifelse(is.na(TestData$X3), 
                       'other',TestData$X3) 

TestData$X3 = factor(TestData$X3,
                      levels = c(1,2,3,'other' ),
                      labels = c('SF', 'S0', 'REJ','other'))


a <- dummyVars(" ~ .",data=TestData)
TestData <- data.frame(predict(a, newdata = TestData))



TestData$X1.icmp <- factor(with(TestData,ifelse((TestData$X1.icmp==1),1,-1)))
TestData$X1.tcp <- factor(with(TestData,ifelse((TestData$X1.tcp==1),1,-1)))
TestData$X1.udp <- factor(with(TestData,ifelse((TestData$X1.udp==1),1,-1)))

TestData$X2.http <- factor(with(TestData,ifelse((TestData$X2.http==1),1,-1)))
TestData$X2.private <- factor(with(TestData,ifelse((TestData$X2.private==1),1,-1)))
TestData$X2.smtp <- factor(with(TestData,ifelse((TestData$X2.smtp==1),1,-1)))
TestData$X2.other <- factor(with(TestData,ifelse((TestData$X2.other==1),1,-1)))

TestData$X3.SF <- factor(with(TestData,ifelse((TestData$X3.SF==1),1,-1)))
TestData$X3.S0<- factor(with(TestData,ifelse((TestData$X3.S0==1),1,-1)))
TestData$X3.REJ <- factor(with(TestData,ifelse((TestData$X3.REJ==1),1,-1)))
TestData$X3.other <- factor(with(TestData,ifelse((TestData$X3.other==1),1,-1)))

##------------------------------scale-------------------------------------------------------------------------------
library(Matrix)
TraindataMat <- data.matrix(sqldf("select * from TestData"))

maxs_train <- as.numeric(apply(TraindataMat, 2, max))
mins_train <- as.numeric(apply(TraindataMat, 2, min))

TraindataMat <- as.data.frame(scale(TraindataMat, center = mins_train, scale = maxs_train - mins_train))

TestData <- TraindataMat

TestData$X1.icmp <- factor(with(TestData,ifelse((TestData$X1.icmp==1),1,-1)))
TestData$X1.tcp <- factor(with(TestData,ifelse((TestData$X1.tcp==1),1,-1)))
TestData$X1.udp <- factor(with(TestData,ifelse((TestData$X1.udp==1),1,-1)))

TestData$X2.http <- factor(with(TestData,ifelse((TestData$X2.http==1),1,-1)))
TestData$X2.private <- factor(with(TestData,ifelse((TestData$X2.private==1),1,-1)))
TestData$X2.smtp <- factor(with(TestData,ifelse((TestData$X2.smtp==1),1,-1)))
TestData$X2.other <- factor(with(TestData,ifelse((TestData$X2.other==1),1,-1)))

TestData$X3.SF <- factor(with(TestData,ifelse((TestData$X3.SF==1),1,-1)))
TestData$X3.S0<- factor(with(TestData,ifelse((TestData$X3.S0==1),1,-1)))
TestData$X3.REJ <- factor(with(TestData,ifelse((TestData$X3.REJ==1),1,-1)))
TestData$X3.other <- factor(with(TestData,ifelse((TestData$X3.other==1),1,-1)))


##--------------------Predict---------------------------------------


Test_Y_pred<- predict(NN_opt3, newdata = TestData[,1:37],type = 'class')


Test_Y_pred <- as.data.frame(as.numeric(Test_Y_pred))





